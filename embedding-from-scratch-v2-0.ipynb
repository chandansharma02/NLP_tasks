{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/word2vec-from-scratch-with-numpy-8786ddd49e72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # obtains tokens with a least 1 alphabet\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())\n",
    "\n",
    "def mapping(tokens):\n",
    "    word_to_id = dict()\n",
    "    id_to_word = dict()\n",
    "\n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        #print (i, token)\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def generate_training_data(tokens, word_to_id, window_size):\n",
    "    N = len(tokens)\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(N):\n",
    "        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n",
    "                   list(range(i + 1, min(N, i + window_size + 1)))\n",
    "        for j in nbr_inds:\n",
    "            X.append(word_to_id[tokens[i]])\n",
    "            #print(X)\n",
    "            Y.append(word_to_id[tokens[j]])\n",
    "            #print(Y)\n",
    "            \n",
    "    X = np.array(X)\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    Y = np.array(Y)\n",
    "    Y = np.expand_dims(Y, axis=0)\n",
    "            \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to download data from Kernal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a download=\"data.csv\" href=\"data:text/csv;base64,LDAKMCwxCjEsMwoyLDQK\" target=\"_blank\">Download CSV file</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the modules we'll need\n",
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import base64\n",
    "\n",
    "# function that takes in a dataframe and creates a text link to  \n",
    "# download it (will only work for files < 2MB or so)\n",
    "def create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n",
    "    csv = df.to_csv()\n",
    "    b64 = base64.b64encode(csv.encode())\n",
    "    payload = b64.decode()\n",
    "    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n",
    "    html = html.format(payload=payload,title=title,filename=filename)\n",
    "    return HTML(html)\n",
    "\n",
    "# create a random sample dataframe\n",
    "df = pd.DataFrame([1,3,4])\n",
    "\n",
    "# create a link to download the dataframe\n",
    "create_download_link(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"After the deduction of the costs of investing, \" \\\n",
    "      \"beating the stock market is a loser's game.\"\n",
    "tokens = tokenize(doc)\n",
    "word_to_id, id_to_word = mapping(tokens)\n",
    "X, Y = generate_training_data(tokens, word_to_id, 3)\n",
    "vocab_size = len(id_to_word)\n",
    "m = Y.shape[1]\n",
    "# turn Y into one hot encoding\n",
    "Y_one_hot = np.zeros((vocab_size, m))\n",
    "Y_one_hot[Y.flatten(), np.arange(m)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wrd_emb(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    vocab_size: int. vocabulary size of your corpus or training data\n",
    "    emb_size: int. word embedding size. How many dimensions to represent each vocabulary\n",
    "    \"\"\"\n",
    "    WRD_EMB = np.random.randn(vocab_size, emb_size) * 0.01\n",
    "    \n",
    "    assert(WRD_EMB.shape == (vocab_size, emb_size))\n",
    "    return WRD_EMB\n",
    "\n",
    "def initialize_dense(input_size, output_size):\n",
    "    \"\"\"\n",
    "    input_size: int. size of the input to the dense layer\n",
    "    output_szie: int. size of the output out of the dense layer\n",
    "    \"\"\"\n",
    "    W = np.random.randn(output_size, input_size) * 0.01\n",
    "    \n",
    "    assert(W.shape == (output_size, input_size))\n",
    "    return W\n",
    "\n",
    "def initialize_parameters(vocab_size, emb_size):\n",
    "    WRD_EMB = initialize_wrd_emb(vocab_size, emb_size)\n",
    "    W = initialize_dense(emb_size, vocab_size)\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters['WRD_EMB'] = WRD_EMB\n",
    "    parameters['W'] = W\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_to_word_vecs(inds, parameters):\n",
    "    \"\"\"\n",
    "    inds: numpy array. shape: (1, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = inds.shape[1]\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    word_vec = WRD_EMB[inds.flatten(), :].T\n",
    "    \n",
    "    assert(word_vec.shape == (WRD_EMB.shape[1], m))\n",
    "    \n",
    "    return word_vec\n",
    "\n",
    "def linear_dense(word_vec, parameters):\n",
    "    \"\"\"\n",
    "    word_vec: numpy array. shape: (emb_size, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = word_vec.shape[1]\n",
    "    W = parameters['W']\n",
    "    Z = np.dot(W, word_vec)\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], m))\n",
    "    \n",
    "    return W, Z\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Z: output out of the dense layer. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis=0, keepdims=True) + 0.001)\n",
    "    \n",
    "    assert(softmax_out.shape == Z.shape)\n",
    "\n",
    "    return softmax_out\n",
    "\n",
    "def forward_propagation(inds, parameters):\n",
    "    word_vec = ind_to_word_vecs(inds, parameters)\n",
    "    W, Z = linear_dense(word_vec, parameters)\n",
    "    softmax_out = softmax(Z)\n",
    "    \n",
    "    caches = {}\n",
    "    caches['inds'] = inds\n",
    "    caches['word_vec'] = word_vec\n",
    "    caches['W'] = W\n",
    "    caches['Z'] = Z\n",
    "    \n",
    "    return softmax_out, caches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(softmax_out, Y):\n",
    "    \"\"\"\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    m = softmax_out.shape[1]\n",
    "    cost = -(1 / m) * np.sum(np.sum(Y * np.log(softmax_out + 0.001), axis=0, keepdims=True), axis=1)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(Y, softmax_out):\n",
    "    \"\"\"\n",
    "    Y: labels of training data. shape: (vocab_size, m)\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    dL_dZ = softmax_out - Y\n",
    "    \n",
    "    assert(dL_dZ.shape == softmax_out.shape)\n",
    "    return dL_dZ\n",
    "\n",
    "def dense_backward(dL_dZ, caches):\n",
    "    \"\"\"\n",
    "    dL_dZ: shape: (vocab_size, m)\n",
    "    caches: dict. results from each steps of forward propagation\n",
    "    \"\"\"\n",
    "    W = caches['W']\n",
    "    word_vec = caches['word_vec']\n",
    "    m = word_vec.shape[1]\n",
    "    \n",
    "    dL_dW = (1 / m) * np.dot(dL_dZ, word_vec.T)\n",
    "    dL_dword_vec = np.dot(W.T, dL_dZ)\n",
    "\n",
    "    assert(W.shape == dL_dW.shape)\n",
    "    assert(word_vec.shape == dL_dword_vec.shape)\n",
    "    \n",
    "    return dL_dW, dL_dword_vec\n",
    "\n",
    "def backward_propagation(Y, softmax_out, caches):\n",
    "    dL_dZ = softmax_backward(Y, softmax_out)\n",
    "    dL_dW, dL_dword_vec = dense_backward(dL_dZ, caches)\n",
    "    \n",
    "    gradients = dict()\n",
    "    gradients['dL_dZ'] = dL_dZ\n",
    "    gradients['dL_dW'] = dL_dW\n",
    "    gradients['dL_dword_vec'] = dL_dword_vec\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, caches, gradients, learning_rate):\n",
    "    vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
    "    inds = caches['inds']\n",
    "    dL_dword_vec = gradients['dL_dword_vec']\n",
    "    m = inds.shape[-1]\n",
    "    \n",
    "    parameters['WRD_EMB'][inds.flatten(), :] -= dL_dword_vec.T * learning_rate\n",
    "\n",
    "    parameters['W'] -= learning_rate * gradients['dL_dW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def skipgram_model_training(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size=256, \n",
    "                            parameters=None, print_cost=False, plot_cost=True):\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    if parameters is None:\n",
    "        parameters = initialize_parameters(vocab_size, emb_size)\n",
    "    \n",
    "    begin_time = datetime.now()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_cost = 0\n",
    "        batch_inds = list(range(0, m, batch_size))\n",
    "#         print(batch_inds)\n",
    "        np.random.shuffle(batch_inds)\n",
    "        for i in batch_inds:\n",
    "            X_batch = X[:, i:i+batch_size]\n",
    "            Y_batch = Y[:, i:i+batch_size]\n",
    "\n",
    "            softmax_out, caches = forward_propagation(X_batch, parameters)\n",
    "            gradients = backward_propagation(Y_batch, softmax_out, caches)\n",
    "            update_parameters(parameters, caches, gradients, learning_rate)\n",
    "            cost = cross_entropy(softmax_out, Y_batch)\n",
    "            epoch_cost += np.squeeze(cost)\n",
    "            \n",
    "        costs.append(epoch_cost)\n",
    "        if print_cost and epoch % (epochs // 500) == 0:\n",
    "            print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))\n",
    "        if epoch % (epochs // 100) == 0:\n",
    "            learning_rate *= 0.98\n",
    "    end_time = datetime.now()\n",
    "    print('training time: {}'.format(end_time - begin_time))\n",
    "            \n",
    "    if plot_cost:\n",
    "        plt.plot(np.arange(epochs), costs)\n",
    "        plt.xlabel('# of epochs')\n",
    "        plt.ylabel('cost')\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 0:00:01.181424\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "paras = skipgram_model_training(X, Y_one_hot, vocab_size, 50, 0.05, 5000, \\\n",
    "                                batch_size=128, parameters=None, print_cost=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.arange(vocab_size)\n",
    "X_test = np.expand_dims(X_test, axis=0)\n",
    "softmax_test, _ = forward_propagation(X_test, paras)\n",
    "top_sorted_inds = np.argsort(softmax_test, axis=0)[-4:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "market's neighbor words: ['is', \"loser's\", 'a', 'beating']\n",
      "is's neighbor words: ['game', 'market', \"loser's\", 'stock']\n",
      "loser's's neighbor words: ['game', 'market', \"loser's\", 'stock']\n",
      "after's neighbor words: ['of', 'the', 'deduction', 'costs']\n",
      "of's neighbor words: ['the', 'of', 'costs', 'beating']\n",
      "the's neighbor words: ['is', 'of', 'beating', 'stock']\n",
      "stock's neighbor words: ['investing', 'a', 'is', 'market']\n",
      "costs's neighbor words: ['beating', 'of', 'deduction', 'the']\n",
      "beating's neighbor words: ['market', 'stock', 'investing', 'costs']\n",
      "a's neighbor words: ['game', 'market', \"loser's\", 'stock']\n",
      "game's neighbor words: ['is', \"loser's\", 'a', 'beating']\n",
      "investing's neighbor words: ['the', 'stock', 'beating', 'of']\n",
      "deduction's neighbor words: ['costs', 'after', 'the', 'beating']\n"
     ]
    }
   ],
   "source": [
    "for input_ind in range(vocab_size):\n",
    "    input_word = id_to_word[input_ind]\n",
    "    output_words = [id_to_word[output_ind] for output_ind in top_sorted_inds[::-1, input_ind]]\n",
    "    print(\"{}'s neighbor words: {}\".format(input_word, output_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
